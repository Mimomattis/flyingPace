#!/bin/bash -l
#
#SBATCH --job-name=runbatch
#SBATCH --output=%x.o%j
#SBATCH --error=%x.e%j
#SBATCH --time=23:59:00
#SBATCH --nodes=1
#SBATCH --partition=a100
#SBATCH --gres=gpu:a100:1
#SBATCH --ntasks-per-node=32
#SBATCH --cpus-per-task=1
#
#SBATCH --distribution=block:block:block
##SBATCH --hint=nomultithread
#SBATCH --export=none
#SBATCH --get-user-env
#
# Partitions:
##SBATCH --gres=gpu:a100:<no_of_gpus> --partition=a100 : nvidia a100/40GB RAM/32CPU nodes per GPU (#SBATCH --ntasks-per-node=32)
##SBATCH --gres=gpu:v100:<no_of_gpus> --partition=v100 : nvidia v100/32GB RAM/8CPU nodes per GPU #SBATCH --ntasks-per-node=8
##SBATCH --gres=gpu:<no_of_gpus> --partition=rtx3080 : nvidia rtx3080/10GB RAM/8CPU nodes per GPU #SBATCH --ntasks-per-node=8
##
## --job-name=runbatch  : name of the run
## --time=23:59:00      : walltime for the run
## --nodes=4            : number of nodes
## --ntasks-per-node=20 : number of MPI processes per node
## --cpus-per-task=1    : number of OMP tasks per MPI process
##
##SBATCH --dependency=afterok:<jobid>
#
# NOTE: Always keep 'cpus-per-task=1'. The 'ntasks-per-node' depends
#       on the chosen queue and has to be equal 'ppn'. You have to use
#       the correct number of cores per node, otherwise your job will
#       be rejected by the batch system.
#
#-----------------------------------------------------------------------
  input="input.yaml"
  potential="{{PREV_POT}}"
#
# set workdir: Only change if you know what you are doing
#
#  workdir="."
   workdir="/dev/shm/${SLURM_JOB_ID}"
#
#-----------------------------------------------------------------------
# nothing needs to be changed below this line
#-----------------------------------------------------------------------
  unset SLURM_EXPORT_ENV
#
# set number of processes per node:
#
  export ppn=${SLURM_NTASKS_PER_NODE}
#
  echo `scontrol show hostnames ${SLURM_JOB_NODELIST}`
  echo ${SLURM_JOB_PARTITION}
  echo "Number of OpenMP threads:     " ${SLURM_CPUS_PER_TASK}
  echo "Number of MPI procs per node: " ${SLURM_NTASKS_PER_NODE}
  echo "Number of MPI processes:      " ${SLURM_NTASKS}
#
# load modules and set conda enviroment
#
module load python
conda activate ace
#
  export OMP_NUM_THREADS=1
  export MKL_NUM_THREADS=1
#  unset I_MPI_PMI_LIBRARY
#  export I_MPI_JOB_RESPECT_PROCESS_PLACEMENT=0
# --report-bindings (for srun)
  export OMPI_MCA_hwloc_base_report_bindings=true
#
# copy to local workdir:
#
  if [ ${workdir} != "." ]; then
    mkdir -p ${workdir}
    cp -a * ${workdir}
    cd ${workdir}
    rm *.o${SLURM_JOB_ID} *.e${SLURM_JOB_ID}
  fi

#
  echo -n "Starting run at: "
  date
  if [ -f "${potential}" ]; then
    pacemaker ${input} -p ${potential}
  else 
    pacemaker ${input}
  fi 

  touch CALC_DONE

  echo -n "Stopping run at: "
  date
#
#
# move results and tidy up:
#
  echo
  if [ ${workdir} != "." ]; then
    cp -a * ${SLURM_SUBMIT_DIR}
    cd ${SLURM_SUBMIT_DIR}
    rm -rf ${workdir}
  fi

conda deactivate 
