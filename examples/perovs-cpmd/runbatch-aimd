#!/bin/bash -l
#
#SBATCH --job-name=runbatch-aimd
#SBATCH --output=%x.o%j
#SBATCH --error=%x.e%j
#SBATCH --time=120:00:00
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=6
#SBATCH --cpus-per-task=12
#SBATCH --partition=batch3
#
#SBATCH --distribution=block:block:block
##SBATCH --hint=nomultithread
#SBATCH --export=none
#SBATCH --get-user-env
#
# Partitions:
##SBATCH --partition=singlenode : fritz    ppn=72
##SBATCH --partition=multinode  : fritz    ppn=72
#
## --job-name=runbatch  : name of the run
## --time=23:59:00      : walltime for the run
## --nodes=4            : number of nodes
## --ntasks-per-node=2  : number of MPI processes per node
## --cpus-per-task=8    : number of OMP tasks per MPI process
##
##SBATCH --dependency=afterok:<jobid>
#
# NOTE: The product of 'ntasks-per-node' times 'cpus-per-task' has
#       to be equal to the number of cores per node (72 for fritz).
#       Typical choices are:
#
#        4 MPI proc/node      6 MPI proc/node      8 MPI proc/node
#       --cpus-per-task=18   --cpus-per-task=12   --cpus-per-task=9
#
# [ 'nodes' and 'ntasks-per-node' can be replaced by 'ntasks' which
#   is the product of both: --ntasks= nodes * ntasks-per-node ]
#
##SBATCH --ntasks=32
##SBATCH --ntasks-per-core=1
##SBATCH --threads-per-core=1
##SBATCH --mem=63000
##SBATCH --hint=compute_bound
#-----------------------------------------------------------------------
  input="INP0"
  output="OUT"
#
# set workdir: Only change if you know what you are doing
#
#  workdir="."
  workdir="/dev/shm/${SLURM_JOB_ID}"
#-----------------------------------------------------------------------
# nothing needs to be changed below this line
#-----------------------------------------------------------------------
  unset SLURM_EXPORT_ENV
#
  echo `scontrol show hostnames ${SLURM_JOB_NODELIST}`
  echo ${SLURM_JOB_PARTITION}
  echo "Number of OpenMP threads:     " ${SLURM_CPUS_PER_TASK}
  echo "Number of MPI procs per node: " ${SLURM_NTASKS_PER_NODE}
  echo "Number of MPI processes:      " ${SLURM_NTASKS}
  echo
#
# load modules
#
  module purge
  module load mkl/2021.4.0
  module load ucx-mt/2.10
  module load hcoll/2.10
  module load openmpi/intel/4.1.2-2021.4.0.3422-hpcx2.10
  module list
#
# enter the SLURM work directory:
#
  cd ${SLURM_SUBMIT_DIR}
#
# set path for pseudopotential library and CPMD executable
#
  export CPMD="/ccc160/bmeyer/bin/cpmd-4.3-tobias.x"
  export SSHFS="/usr/bin/sshfs"
  export PAR_RUN="srun"
#  export PAR_RUN="mpirun"
  export PP_LIBRARY_PATH="/ccc160/bmeyer/pot.cpmd/functional-PBEsol/"
  which ${PAR_RUN}
#
  unset KMP_AFFINITY
  export KMP_WARNINGS=0
  export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}
  export OMP_PLACES=cores
  export OMP_PROC_BIND=close,close
  export OMP_WAIT_POLICY='ACTIVE'
  export OMP_DYNAMIC=false
  export OMP_ACTIVE_LEVELS=1
  export OMP_MAX_ACTIVE_LEVELS=1
  export MKL_DYNAMIC=0
  export MKL_NUM_THREADS=${OMP_NUM_THREADS}
# --report-bindings (for srun)
  export OMPI_MCA_hwloc_base_report_bindings=true
#  export OMP_WARNINGS=0
#
# copy to local workdir:
#
  if [ ${workdir} != "." ]; then
    mkdir -p ${workdir}
    cp -a * ${workdir}
#    mkdir -p tmp-work
#    ssh ccc160 ${SSHFS} ${USER}@${HOSTNAME}:${workdir} ${SLURM_SUBMIT_DIR}/tmp-work
    echo ${USER}@${HOSTNAME}:${workdir}
    echo ${SLURM_SUBMIT_DIR}/tmp-work
    cd ${workdir}
    rm *.o${SLURM_JOB_ID} *.e${SLURM_JOB_ID}
  fi
#
# set automatic run stop 1800s before end of walltime limit:
#
  SLEEPTIME=$(grep time $0 | head -1 | sed 's/.*time=//g'| \
              awk -F: '{print -1800+3600*$1+$2*60+$3}')
  echo "auto stop in ${SLEEPTIME} seconds"
  (sleep ${SLEEPTIME} ; touch ${workdir}/EXIT ; echo -n "touched EXIT at "; date) &
#
  echo -n "Starting run at: "
  date

#mpirun:
#  export MFLAG="--map-by ppr:${SLURM_NTASKS_PER_NODE}:node:PE=${OMP_NUM_THREADS} \
#                --bind-to core --report-bindings"
#  ${PAR_RUN} ${MFLAG} ${CPMD} ${input} ${PP_LIBRARY_PATH} > ${output}

#srun:
#  export MFLAG="--hint=nomultithread"

  ${PAR_RUN} ${CPMD} ${input} ${PP_LIBRARY_PATH} > ${output}

  echo -n "Stopping run at: "
  date
#
# move results and tidy up:
#
  echo
  if [ ${workdir} != "." ]; then
    cp -a * ${SLURM_SUBMIT_DIR}
    cd ${SLURM_SUBMIT_DIR}
#    ssh ccc160 fusermount -uz ${SLURM_SUBMIT_DIR}/tmp-work
#    rmdir tmp-work
    rm -rf ${workdir}
  fi
#

 touch CALC_DONE
