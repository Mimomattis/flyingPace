#!/bin/bash -l
#
#SBATCH --job-name=runbatch-ace
#SBATCH --output=%x.o%j
#SBATCH --error=%x.e%j
#SBATCH --time=23:59:00
#SBATCH --nodes=1
#SBATCH --partition=a100
#SBATCH --gres=gpu:a100:1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=32
#
#-----------------------------------------------------------------------
  input="input.yaml"
  potential="cont.yaml"
#
# set workdir: Only change if you know what you are doing
#
#  workdir="."
   workdir="/dev/shm/${SLURM_JOB_ID}"
#
#-----------------------------------------------------------------------
# nothing needs to be changed below this line
#-----------------------------------------------------------------------
  unset SLURM_EXPORT_ENV
#
# set number of processes per node:
#
  export ppn=${SLURM_NTASKS_PER_NODE}
#
  echo `scontrol show hostnames ${SLURM_JOB_NODELIST}`
  echo ${SLURM_JOB_PARTITION}
  echo "Number of OpenMP threads:     " ${SLURM_CPUS_PER_TASK}
  echo "Number of MPI procs per node: " ${SLURM_NTASKS_PER_NODE}
  echo "Number of MPI processes:      " ${SLURM_NTASKS}
#
# load modules and set conda enviroment
#
module load python
conda activate ace
#
  export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}
  export OMPI_MCA_hwloc_base_report_bindings=true
#
# copy to local workdir:
#
  if [ ${workdir} != "." ]; then
    mkdir -p ${workdir}
    cp -a * ${workdir}
    cd ${workdir}
    rm *.o${SLURM_JOB_ID} *.e${SLURM_JOB_ID}
  fi

#
  echo -n "Starting run at: "
  date
  if [ -f "${potential}" ]; then
    pacemaker ${input} -p ${potential}
  else 
    pacemaker ${input}
  fi 

  touch CALC_DONE

  echo -n "Stopping run at: "
  date
#
#
# move results and tidy up:
#
  echo
  if [ ${workdir} != "." ]; then
    cp -a * ${SLURM_SUBMIT_DIR}
    cd ${SLURM_SUBMIT_DIR}
    rm -rf ${workdir}
  fi

conda deactivate 
